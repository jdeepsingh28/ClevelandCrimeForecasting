{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51a7dd18-7fa8-481c-865c-58fed8238234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_crime_data(crime):\n",
    "    # Ensure 'offense_date' is in datetime format\n",
    "    crime['offense_date'] = pd.to_datetime(crime['offense_date'], errors='coerce')\n",
    "    crime.dropna(subset=['offense_date'], inplace=True)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['census_tract_geoid', 'census_block_group', 'census_bg_geoid', 'census_block_geoid', 'std_parcelpin',\n",
    "                    'address_public', 'object_id', 'primary_key', 'case_number', 'reported_date', 'dow_name', 'statute', \n",
    "                    'stat_desc', 'date', 'days_ago', 'geoid', 'city', 'zip', 'ward', 'primary_key', 'district', 'time_group', \n",
    "                    'census_tract', 'time_block']\n",
    "    crime = crime.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    # Handle date discrepancies\n",
    "    crime['extracted_month'] = crime['offense_date'].dt.month\n",
    "    crime['extracted_day'] = crime['offense_date'].dt.day\n",
    "    crime = crime.drop(columns=['offense_month', 'offense_day'], errors='ignore')\n",
    "\n",
    "    # Drop duplicates\n",
    "    crime = crime.drop_duplicates()\n",
    "\n",
    "    # Encode cyclical features\n",
    "    crime['dow'] = crime['offense_date'].dt.weekday + 1  # Monday=1, Sunday=7\n",
    "    crime['dow_sin'] = np.sin(2 * np.pi * (crime['dow'] - 1) / 7)\n",
    "    crime['dow_cos'] = np.cos(2 * np.pi * (crime['dow'] - 1) / 7)\n",
    "    crime['day_sin'] = np.sin(2 * np.pi * (crime['extracted_day'] - 1) / 31)\n",
    "    crime['day_cos'] = np.cos(2 * np.pi * (crime['extracted_day'] - 1) / 31)\n",
    "    crime['month_sin'] = np.sin(2 * np.pi * (crime['extracted_month'] - 1) / 12)\n",
    "    crime['month_cos'] = np.cos(2 * np.pi * (crime['extracted_month'] - 1) / 12)\n",
    "    crime['hour_sin'] = np.sin(2 * np.pi * crime['hour_of_day'] / 24)\n",
    "    crime['hour_cos'] = np.cos(2 * np.pi * crime['hour_of_day'] / 24)\n",
    "\n",
    "    # Drop original cyclical columns\n",
    "    crime = crime.drop(columns=['dow', 'hour_of_day', 'extracted_month', 'extracted_day'], errors='ignore')\n",
    "\n",
    "    # Label Encoding\n",
    "    le_ucr_desc = LabelEncoder()\n",
    "    crime['ucr_desc_numeric'] = le_ucr_desc.fit_transform(crime['ucr_desc'])\n",
    "    le_offense_year = LabelEncoder()\n",
    "    crime['offense_year_numeric'] = le_offense_year.fit_transform(crime['offense_year'])\n",
    "    le_census_block = LabelEncoder()\n",
    "    crime['census_block_numeric'] = le_census_block.fit_transform(crime['census_block'])\n",
    "\n",
    "    # Drop original categorical columns\n",
    "    crime = crime.drop(columns=['ucr_desc', 'offense_year', 'census_block'], errors='ignore')\n",
    "\n",
    "    # Feature Engineering\n",
    "    crime['week_of_year'] = crime['offense_date'].dt.isocalendar().week\n",
    "    crime['temp_range'] = crime['temp_max'] - crime['temp_min']\n",
    "\n",
    "    # Interaction Features\n",
    "    crime['week_precipitation_interaction'] = crime['week_of_year'] * crime['precipitation_sum']\n",
    "    crime['daylight_precipitation_interaction'] = crime['daylight_duration'] * crime['precipitation_sum']\n",
    "    crime['block_week_interaction'] = crime['census_block_numeric'] * crime['week_of_year']\n",
    "    crime['block_temp_max_interaction'] = crime['census_block_numeric'] * crime['temp_max']\n",
    "    crime['temp_range_precipitation_interaction'] = crime['temp_range'] * crime['precipitation_sum']\n",
    "    crime['precipitation_sum_hours_interaction'] = crime['precipitation_sum'] * crime['precipitation_hours']\n",
    "\n",
    "    # Holiday Feature\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    holidays = cal.holidays(start=crime['offense_date'].min(), end=crime['offense_date'].max())\n",
    "    crime['is_holiday'] = crime['offense_date'].dt.normalize().isin(holidays).astype(int)\n",
    "\n",
    "    return crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27e6e88a-f9ef-4a98-a14a-c8c4b96a9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add weekly crime count lag feature\n",
    "def add_weekly_lag_features(df, target_column, lags):\n",
    "    df = df.sort_values(['census_block_numeric', 'date'])\n",
    "    for lag in lags:\n",
    "        lag_column_name = f\"{target_column}_lag{lag}w\"\n",
    "        df[lag_column_name] = df.groupby('census_block_numeric')[target_column].shift(lag)\n",
    "    return df\n",
    "\n",
    "# Function to add weekly crime count rolling feature\n",
    "def add_rolling_features(df, target_column, windows):\n",
    "    df = df.sort_values(['census_block_numeric', 'date'])\n",
    "    for window in windows:\n",
    "        rolling_column_name = f\"{target_column}_rolling{window}\"\n",
    "        df[rolling_column_name] = df.groupby('census_block_numeric')[target_column].transform(\n",
    "            lambda x: x.shift(1).rolling(window=window).mean()\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10ad2e2c-8871-4fb1-b7dc-ab155696a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series(data):\n",
    "    # Preprocess the data\n",
    "    data = preprocess_crime_data(data)\n",
    "    \n",
    "    # Set 'offense_date' as index for grouping\n",
    "    data.set_index('offense_date', inplace=True)\n",
    "\n",
    "    # Define the aggregation functions for each feature\n",
    "    agg_dict = {\n",
    "        'dow_sin': 'mean',\n",
    "        'dow_cos': 'mean',\n",
    "        'day_sin': 'mean',\n",
    "        'day_cos': 'mean',\n",
    "        'month_sin': 'mean',\n",
    "        'month_cos': 'mean',\n",
    "        'temp_max': 'mean',\n",
    "        'temp_min': 'mean',\n",
    "        'temp_range': 'mean',\n",
    "        'daylight_duration': 'mean',\n",
    "        'precipitation_sum': 'mean',\n",
    "        'precipitation_hours': 'mean',\n",
    "        'week_precipitation_interaction': 'mean',\n",
    "        'daylight_precipitation_interaction': 'mean',\n",
    "        'block_week_interaction': 'mean',\n",
    "        'block_temp_max_interaction': 'mean',\n",
    "        'temp_range_precipitation_interaction': 'mean',\n",
    "        'is_holiday': 'mean',\n",
    "        'week_of_year': 'first',\n",
    "    }\n",
    "\n",
    "    # Group by 'census_block_numeric' and weekly intervals\n",
    "    time_series = data.groupby(['census_block_numeric', pd.Grouper(freq='W')]).agg(agg_dict).reset_index()\n",
    "\n",
    "    # Calculate the weekly crime count per census block\n",
    "    time_series['Crime_Count_W'] = data.groupby(['census_block_numeric', pd.Grouper(freq='W')]).size().values\n",
    "\n",
    "    # Rename 'offense_date' to 'date' for ease of use\n",
    "    time_series.rename(columns={'offense_date': 'date'}, inplace=True)\n",
    "\n",
    "    # Sort the time series\n",
    "    time_series = time_series.sort_values(by=['census_block_numeric', 'date'])\n",
    "\n",
    "    # Add lag and rolling features\n",
    "    lag_intervals = [1, 4, 12, 24, 52]\n",
    "    time_series = add_weekly_lag_features(time_series, target_column='Crime_Count_W', lags=lag_intervals)\n",
    "    rolling_windows = [2, 3, 4, 8, 15]\n",
    "    time_series = add_rolling_features(time_series, target_column='Crime_Count_W', windows=rolling_windows)\n",
    "    \n",
    "    # Create interaction features\n",
    "    for window in rolling_windows:\n",
    "        time_series[f'week_rolling{window}_interaction'] = (\n",
    "            time_series['week_of_year'] * time_series[f'Crime_Count_W_rolling{window}']\n",
    "        )\n",
    "        time_series[f'temp_rolling{window}_interaction'] = (\n",
    "            time_series['temp_range'] * time_series[f'Crime_Count_W_rolling{window}']\n",
    "        )\n",
    "        time_series[f'precip_rolling{window}_interaction'] = (\n",
    "            time_series['precipitation_hours'] * time_series[f'Crime_Count_W_rolling{window}']\n",
    "        )\n",
    "\n",
    "    for lag in lag_intervals:\n",
    "        time_series[f'daylight_lag{lag}_interaction'] = (\n",
    "            time_series['daylight_duration'] * time_series[f'Crime_Count_W_lag{lag}w']\n",
    "        )\n",
    "        \n",
    "    time_series = time_series.dropna()\n",
    "\n",
    "    # Define features to apply log transformation\n",
    "    log_features = [\n",
    "        'Crime_Count_W', \n",
    "        'Crime_Count_W_lag1w', 'Crime_Count_W_lag4w',\n",
    "        'Crime_Count_W_lag12w', 'Crime_Count_W_lag24w', 'Crime_Count_W_lag52w',\n",
    "        'Crime_Count_W_rolling2', 'Crime_Count_W_rolling3', 'Crime_Count_W_rolling4',\n",
    "        'Crime_Count_W_rolling8', 'Crime_Count_W_rolling15',\n",
    "        'week_rolling2_interaction', 'temp_rolling2_interaction',\n",
    "        'precip_rolling2_interaction', 'week_rolling3_interaction',\n",
    "        'temp_rolling3_interaction', 'precip_rolling3_interaction',\n",
    "        'week_rolling4_interaction', 'temp_rolling4_interaction',\n",
    "        'precip_rolling4_interaction', 'week_rolling8_interaction',\n",
    "        'temp_rolling8_interaction', 'precip_rolling8_interaction',\n",
    "        'week_rolling15_interaction', 'temp_rolling15_interaction',\n",
    "        'precip_rolling15_interaction', 'daylight_lag1_interaction',\n",
    "        'daylight_lag4_interaction', 'daylight_lag12_interaction',\n",
    "        'daylight_lag24_interaction', 'daylight_lag52_interaction'\n",
    "    ]\n",
    "    \n",
    "    # Apply log1p transformation to specified features\n",
    "    time_series[log_features] = time_series[log_features].apply(lambda x: np.log1p(x))\n",
    "\n",
    "    # Reset index\n",
    "    time_series.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "080f5417-a2d0-4d6d-a977-532eb3fe331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_451760\\2295305767.py:7: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  crime = pd.read_csv(r\"\\Users\\singh\\Downloads\\crime_with_weather.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "# Load your data\n",
    "crime = pd.read_csv(r\"\\Users\\singh\\Downloads\\crime_with_weather.csv\")\n",
    "\n",
    "# Create time series\n",
    "crime_time_series = create_time_series(crime)\n",
    "crime_time_series.to_csv(\"crime_time_series_2\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c10dfe26-de78-4602-8e0f-2ebaef432d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_future_data(time_series):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "    \n",
    "    # Get the last date and add one week\n",
    "    last_date = time_series['date'].max()\n",
    "    future_date = last_date + pd.Timedelta(weeks=1)\n",
    "\n",
    "    # Create future data for each census block\n",
    "    census_blocks = time_series['census_block_numeric'].unique()\n",
    "    future_data = pd.DataFrame({\n",
    "        'census_block_numeric': census_blocks,\n",
    "        'date': future_date\n",
    "    })\n",
    "\n",
    "    # Merge with historical data to get features\n",
    "    combined_data = pd.concat([time_series, future_data], ignore_index=True)\n",
    "    combined_data = combined_data.sort_values(['census_block_numeric', 'date']).reset_index(drop=True)\n",
    "\n",
    "    # Recompute lag and rolling features\n",
    "    lag_intervals = [1, 4, 12, 24, 52]\n",
    "    combined_data = add_weekly_lag_features(combined_data, target_column='Crime_Count_W', lags=lag_intervals)\n",
    "    rolling_windows = [2, 3, 4, 8, 15]\n",
    "    combined_data = add_rolling_features(combined_data, target_column='Crime_Count_W', windows=rolling_windows)\n",
    "\n",
    "    # Identify future data indices\n",
    "    future_indices = combined_data['date'] == future_date\n",
    "\n",
    "    # Recompute 'week_of_year' for future date\n",
    "    combined_data.loc[future_indices, 'week_of_year'] = future_date.isocalendar()[1]\n",
    "\n",
    "    # Compute cyclical features based on 'date'\n",
    "    combined_data['dow'] = combined_data['date'].dt.weekday + 1  # Monday=1, Sunday=7\n",
    "    combined_data['dow_sin'] = np.sin(2 * np.pi * (combined_data['dow'] - 1) / 7)\n",
    "    combined_data['dow_cos'] = np.cos(2 * np.pi * (combined_data['dow'] - 1) / 7)\n",
    "    combined_data['day'] = combined_data['date'].dt.day\n",
    "    combined_data['day_sin'] = np.sin(2 * np.pi * (combined_data['day'] - 1) / 31)\n",
    "    combined_data['day_cos'] = np.cos(2 * np.pi * (combined_data['day'] - 1) / 31)\n",
    "    combined_data['month'] = combined_data['date'].dt.month\n",
    "    combined_data['month_sin'] = np.sin(2 * np.pi * (combined_data['month'] - 1) / 12)\n",
    "    combined_data['month_cos'] = np.cos(2 * np.pi * (combined_data['month'] - 1) / 12)\n",
    "\n",
    "    # Compute 'is_holiday' for all dates\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    holidays = cal.holidays(start=combined_data['date'].min(), end=combined_data['date'].max())\n",
    "    combined_data['is_holiday'] = combined_data['date'].dt.normalize().isin(holidays).astype(int)\n",
    "\n",
    "    # For features that need to be estimated for the future week, use historical averages\n",
    "    features_to_estimate = [\n",
    "        'temp_max', 'temp_min', 'precipitation_hours', 'precipitation_sum', 'daylight_duration'\n",
    "    ]\n",
    "\n",
    "    for feature in features_to_estimate:\n",
    "        if feature in combined_data.columns:\n",
    "            # Use historical average\n",
    "            historical_mean = combined_data.loc[~future_indices, feature].mean()\n",
    "            combined_data.loc[future_indices, feature] = historical_mean\n",
    "\n",
    "    # Recompute 'temp_range' after filling 'temp_max' and 'temp_min'\n",
    "    combined_data['temp_range'] = combined_data['temp_max'] - combined_data['temp_min']\n",
    "\n",
    "    # Recompute interaction features\n",
    "    combined_data['week_precipitation_interaction'] = combined_data['week_of_year'] * combined_data['precipitation_sum']\n",
    "    combined_data['daylight_precipitation_interaction'] = combined_data['daylight_duration'] * combined_data['precipitation_sum']\n",
    "    combined_data['block_week_interaction'] = combined_data['census_block_numeric'] * combined_data['week_of_year']\n",
    "    combined_data['block_temp_max_interaction'] = combined_data['census_block_numeric'] * combined_data['temp_max']\n",
    "    combined_data['temp_range_precipitation_interaction'] = combined_data['temp_range'] * combined_data['precipitation_sum']\n",
    "    combined_data['precipitation_sum_hours_interaction'] = combined_data['precipitation_sum'] * combined_data['precipitation_hours']\n",
    "\n",
    "    # Create interaction features involving lag and rolling features\n",
    "    for window in rolling_windows:\n",
    "        combined_data[f'week_rolling{window}_interaction'] = (\n",
    "            combined_data['week_of_year'] * combined_data[f'Crime_Count_W_rolling{window}']\n",
    "        )\n",
    "        combined_data[f'temp_rolling{window}_interaction'] = (\n",
    "            combined_data['temp_range'] * combined_data[f'Crime_Count_W_rolling{window}']\n",
    "        )\n",
    "        combined_data[f'precip_rolling{window}_interaction'] = (\n",
    "            combined_data['precipitation_hours'] * combined_data[f'Crime_Count_W_rolling{window}']\n",
    "        )\n",
    "\n",
    "    for lag in lag_intervals:\n",
    "        combined_data[f'daylight_lag{lag}_interaction'] = (\n",
    "            combined_data['daylight_duration'] * combined_data[f'Crime_Count_W_lag{lag}w']\n",
    "        )\n",
    "\n",
    "    # Apply log transformation to the relevant features\n",
    "    log_features = [\n",
    "        'Crime_Count_W', \n",
    "        'Crime_Count_W_lag1w', 'Crime_Count_W_lag4w',\n",
    "        'Crime_Count_W_lag12w', 'Crime_Count_W_lag24w', 'Crime_Count_W_lag52w',\n",
    "        'Crime_Count_W_rolling2', 'Crime_Count_W_rolling3', 'Crime_Count_W_rolling4',\n",
    "        'Crime_Count_W_rolling8', 'Crime_Count_W_rolling15',\n",
    "        'week_rolling2_interaction', 'temp_rolling2_interaction',\n",
    "        'precip_rolling2_interaction', 'week_rolling3_interaction',\n",
    "        'temp_rolling3_interaction', 'precip_rolling3_interaction',\n",
    "        'week_rolling4_interaction', 'temp_rolling4_interaction',\n",
    "        'precip_rolling4_interaction', 'week_rolling8_interaction',\n",
    "        'temp_rolling8_interaction', 'precip_rolling8_interaction',\n",
    "        'week_rolling15_interaction', 'temp_rolling15_interaction',\n",
    "        'precip_rolling15_interaction', 'daylight_lag1_interaction',\n",
    "        'daylight_lag4_interaction', 'daylight_lag12_interaction',\n",
    "        'daylight_lag24_interaction', 'daylight_lag52_interaction'\n",
    "    ]\n",
    "\n",
    "    # Ensure all log features are present\n",
    "    existing_log_features = [feature for feature in log_features if feature in combined_data.columns]\n",
    "\n",
    "    combined_data[existing_log_features] = combined_data[existing_log_features].apply(lambda x: np.log1p(x))\n",
    "\n",
    "    # Extract future data\n",
    "    future_data_prepared = combined_data[future_indices].copy()\n",
    "\n",
    "    # Clean up unnecessary columns if needed\n",
    "    columns_to_drop = ['dow', 'day', 'month']  # Drop if these are not needed\n",
    "    future_data_prepared = future_data_prepared.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    return future_data_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00c5d0e7-63c6-41c6-9cd1-6569ae34e675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['census_block_numeric', 'date', 'dow_sin', 'dow_cos', 'day_sin',\n",
       "       'day_cos', 'month_sin', 'month_cos', 'temp_max', 'temp_min',\n",
       "       'temp_range', 'daylight_duration', 'precipitation_sum',\n",
       "       'precipitation_hours', 'week_precipitation_interaction',\n",
       "       'daylight_precipitation_interaction', 'block_week_interaction',\n",
       "       'block_temp_max_interaction', 'temp_range_precipitation_interaction',\n",
       "       'is_holiday', 'week_of_year', 'Crime_Count_W', 'Crime_Count_W_lag1w',\n",
       "       'Crime_Count_W_lag4w', 'Crime_Count_W_lag12w', 'Crime_Count_W_lag24w',\n",
       "       'Crime_Count_W_lag52w', 'Crime_Count_W_rolling2',\n",
       "       'Crime_Count_W_rolling3', 'Crime_Count_W_rolling4',\n",
       "       'Crime_Count_W_rolling8', 'Crime_Count_W_rolling15',\n",
       "       'week_rolling2_interaction', 'temp_rolling2_interaction',\n",
       "       'precip_rolling2_interaction', 'week_rolling3_interaction',\n",
       "       'temp_rolling3_interaction', 'precip_rolling3_interaction',\n",
       "       'week_rolling4_interaction', 'temp_rolling4_interaction',\n",
       "       'precip_rolling4_interaction', 'week_rolling8_interaction',\n",
       "       'temp_rolling8_interaction', 'precip_rolling8_interaction',\n",
       "       'week_rolling15_interaction', 'temp_rolling15_interaction',\n",
       "       'precip_rolling15_interaction', 'daylight_lag1_interaction',\n",
       "       'daylight_lag4_interaction', 'daylight_lag12_interaction',\n",
       "       'daylight_lag24_interaction', 'daylight_lag52_interaction',\n",
       "       'precipitation_sum_hours_interaction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_data = prepare_future_data(crime_time_series)\n",
    "future_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89087034-0658-4fb3-bf2c-859d6fddeed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data.to_csv(\"future_data\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
